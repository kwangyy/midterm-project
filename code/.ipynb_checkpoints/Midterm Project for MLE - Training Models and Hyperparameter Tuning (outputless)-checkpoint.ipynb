{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f56cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\User\\Desktop\\midterm-project\\midterm-project\\data\\cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d354e",
   "metadata": {},
   "source": [
    "## Preparation of Data \n",
    "\n",
    "This is a continuation of the previous notebook. If you have not seen the previous data, please refer to [the previous notebook!](https://www.kaggle.com/kwangyangchia/midterm-project-for-mle) Similarly, there is a notebook that is stored in the github repository that you can look at. The previous notebook was mostly for cleaning some data and visualisation, but I have decided to encode the data here instead to make the workflow a bit cleaner and so that I can control the modules' versions on the environment that I'm working in. \n",
    "\n",
    "If you recall from the dataset, there are numerical and categorical variables. The categorical variables need to be encoded in order for it to be used in the dataset itself.\n",
    "\n",
    "However, for certain categorical variables, we will be using LabelEncoder isntead of using a OneHotEncoder (or in this case, a DictVectorizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing code from the previous notebook\n",
    "categorical = []\n",
    "numerical = []\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':\n",
    "        categorical.append(column)\n",
    "    else:\n",
    "        numerical.append(column)\n",
    "        \n",
    "# Finding categorical variables with more than 20 unique values\n",
    "\n",
    "for column in categorical:\n",
    "    if data[column].nunique() > 20:\n",
    "        print(column)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c7113",
   "metadata": {},
   "source": [
    "## Difference between a label encoder and a dict vectorizer/one-hot encoder\n",
    "A label encoder associates one value to a number and is suitable for some variables that have more than a number of unique values. This is compares to a One Hot Encoder instead, whereby new variables are created based on whether they have that value or not. This will only change the values of the categorical data, and not the whole dataset. \n",
    "\n",
    "We will be using the dict vectorizer once we have split the data. \n",
    "\n",
    "e.g. If we have datapoints ['Amsterdam', 'Paris', 'Amsterdam', 'Berlin'], we will be converting that to [0,1,0,2] using a label encoder. This is compared to a OneHotEncoder where there will be columns \"Amsterdam\", \"Paris\" and \"Berlin\" with either 0/1.\n",
    "\n",
    "For more information, do read the [Label Encoder documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) for details! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoded_columns = ['area', 'country']\n",
    "le = LabelEncoder()\n",
    "\n",
    "for column in label_encoded_columns:\n",
    "    data[column] = le.fit_transform(data[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bd8a8",
   "metadata": {},
   "source": [
    "## Train Test Split of the Data \n",
    "\n",
    "We will be splitting the dataset into 60%/20%/20%, in terms of the training dataset, the validation dataset, and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_train, data_test = train_test_split(data, random_state = 25, test_size = 0.2, shuffle = True)\n",
    "data_train, data_val = train_test_split(data_full_train, random_state = 25, test_size = 0.25, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294614d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.reset_index(drop = True)\n",
    "data_val = data_val.reset_index(drop = True)\n",
    "data_test = data_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['totalyearlycompensation']\n",
    "y_val = data_val['totalyearlycompensation']\n",
    "y_test = data_test['totalyearlycompensation']\n",
    "\n",
    "del data_train['totalyearlycompensation']\n",
    "del data_val['totalyearlycompensation']\n",
    "del data_test['totalyearlycompensation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646725e",
   "metadata": {},
   "source": [
    "## Training the models - 1st round\n",
    "\n",
    "We will be training the data on a few different regression models and judging the models based on root mean squared error (RMSE). \n",
    "\n",
    "There will not be any form of hyperparameter tuning just yet, we're just looking for the best regression model first. \n",
    "\n",
    "Since the dataset itself isn't too big, we will be using cross-validation in order to better judge the results.\n",
    "\n",
    "I have decided to round off the RMSE and the standard deviation (SD) to 5 digits mainly to highlight any difference between the normal linear regression and Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function \n",
    "def train(data_train, y_train, model):\n",
    "    dicts = data_train.to_dict(orient = 'records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse = False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return dv, model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b911419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(data_val, y_val, model):\n",
    "    dicts = data_val.to_dict(orient = 'records')\n",
    "    \n",
    "    X_val = dv.fit_transform(dicts)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b15416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_pred, y_val):\n",
    "    score = float(mean_squared_error(y_pred, y_val))** 0.5 \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41388b53",
   "metadata": {},
   "source": [
    "## Basic Linear Regression\n",
    "We will start off with basic linear regression as a baseline for the rest of the models. After all, it is the simplest regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cfe891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b07524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "n_splits = 5\n",
    "scores = []\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    \n",
    "for train_idx, val_idx in kfold.split(data_full_train):\n",
    "    # data splitting\n",
    "    data_train = data_full_train.iloc[train_idx]\n",
    "    data_val = data_full_train.iloc[val_idx]\n",
    "        \n",
    "    # y values\n",
    "    y_train = data_train['totalyearlycompensation']\n",
    "    y_val = data_val['totalyearlycompensation']\n",
    "    \n",
    "    del data_train['totalyearlycompensation']\n",
    "    del data_val['totalyearlycompensation']\n",
    "        \n",
    "    # training and predicting\n",
    "    dv, model = train(data_train, y_train, linreg)\n",
    "    y_pred = predict(data_val, dv, model)\n",
    "    \n",
    "    score = rmse(y_pred, y_val)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('RMSE for model %s: %.5f +- %.5f' % (model, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79afde",
   "metadata": {},
   "source": [
    "The baseline for the regression is having a RMSE of 0.41078, and having a SD of about 0.003. I think that's a pretty good result, even for a base model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179ea4e",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Ridge Regression is basically Linear Regression with regularization. To find out more about regularization, you can refer to [my notebook under 2.13](https://www.kaggle.com/kwangyangchia/notebook-for-lesson-2-mle) or refer to the [wikipedia page](https://en.wikipedia.org/wiki/Regularization_(mathematics))\n",
    "\n",
    "(yes i know i plugged my own notebook XD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3909459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "ridge = Ridge()\n",
    "n_splits = 5\n",
    "scores = []\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    \n",
    "for train_idx, val_idx in kfold.split(data_full_train):\n",
    "    # data splitting\n",
    "    data_train = data_full_train.iloc[train_idx]\n",
    "    data_val = data_full_train.iloc[val_idx]\n",
    "        \n",
    "    # y values\n",
    "    y_train = data_train['totalyearlycompensation']\n",
    "    y_val = data_val['totalyearlycompensation']\n",
    "    \n",
    "    del data_train['totalyearlycompensation']\n",
    "    del data_val['totalyearlycompensation']\n",
    "        \n",
    "    # training and predicting\n",
    "    dv, model = train(data_train, y_train, ridge)\n",
    "    y_pred = predict(data_val, dv, model)\n",
    "    \n",
    "    score = rmse(y_pred, y_val)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('RMSE for model %s: %.5f +- %.5f' % (model, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fcc71",
   "metadata": {},
   "source": [
    "As we can tell, even with cross-validation, the RMSE and SD is exactly the same when rounded off to 5 significant figures, so we can tell that regularization won't significantly change any results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bedf0",
   "metadata": {},
   "source": [
    "## Random Forest Regressor \n",
    "\n",
    "If you recall what a Random Forest Classifier is, a Random Forest Regressor works the same way but instead of predicting a probability, it predicts a value instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75067d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "n_splits = 5\n",
    "scores = []\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    \n",
    "for train_idx, val_idx in kfold.split(data_full_train):\n",
    "    # data splitting\n",
    "    data_train = data_full_train.iloc[train_idx]\n",
    "    data_val = data_full_train.iloc[val_idx]\n",
    "        \n",
    "    # y values\n",
    "    y_train = data_train['totalyearlycompensation']\n",
    "    y_val = data_val['totalyearlycompensation']\n",
    "    \n",
    "    del data_train['totalyearlycompensation']\n",
    "    del data_val['totalyearlycompensation']\n",
    "        \n",
    "    # training and predicting\n",
    "    dv, model = train(data_train, y_train, rfr)\n",
    "    y_pred = predict(data_val, dv, model)\n",
    "    \n",
    "    score = rmse(y_pred, y_val)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('RMSE for model %s: %.5f +- %.5f' % (model, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c04323",
   "metadata": {},
   "source": [
    "We can tell that the RMSE has decreased by a bit, and the standard deviation between the scores has decreased, even in the slightest degree. Seems like Random Forest is working better than the normal linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcefdc3",
   "metadata": {},
   "source": [
    "## XGB Regressor\n",
    "XGBoost is a gradient descent algorithm that was covered back in Lesson 6. While we used the Classifier for that lesson, this one uses a Regressor instead to give us a predicted value instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d22877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "n_splits = 5\n",
    "scores = []\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    \n",
    "for train_idx, val_idx in kfold.split(data_full_train):\n",
    "    # data splitting\n",
    "    data_train = data_full_train.iloc[train_idx]\n",
    "    data_val = data_full_train.iloc[val_idx]\n",
    "        \n",
    "    # y values\n",
    "    y_train = data_train['totalyearlycompensation']\n",
    "    y_val = data_val['totalyearlycompensation']\n",
    "    \n",
    "    del data_train['totalyearlycompensation']\n",
    "    del data_val['totalyearlycompensation']\n",
    "        \n",
    "    # training and predicting\n",
    "    dv, model = train(data_train, y_train, xgb)\n",
    "    y_pred = predict(data_val, dv, model)\n",
    "    \n",
    "    score = rmse(y_pred, y_val)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('RMSE for XGBRegressor(): %.5f +- %.5f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdb1be",
   "metadata": {},
   "source": [
    "The XGBoost algorithm gives us the best scores, along with the best SD so far! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6cfd85",
   "metadata": {},
   "source": [
    "## CatBoost Regressor\n",
    "CatBoost is also another gradient boosting algorithm that is used in Machine Learning. It is faster and better performing than other gradient boosting algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cbr = CatBoostRegressor(silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "n_splits = 5\n",
    "scores = []\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    \n",
    "for train_idx, val_idx in kfold.split(data_full_train):\n",
    "    # data splitting\n",
    "    data_train = data_full_train.iloc[train_idx]\n",
    "    data_val = data_full_train.iloc[val_idx]\n",
    "        \n",
    "    # y values\n",
    "    y_train = data_train['totalyearlycompensation']\n",
    "    y_val = data_val['totalyearlycompensation']\n",
    "    \n",
    "    del data_train['totalyearlycompensation']\n",
    "    del data_val['totalyearlycompensation']\n",
    "        \n",
    "    # training and predicting\n",
    "    dv, model = train(data_train, y_train, cbr)\n",
    "    y_pred = predict(data_val, dv, model)\n",
    "    \n",
    "    score = rmse(y_pred, y_val)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('RMSE for CatBoostRegressor(): %.5f +- %.5f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fdc17",
   "metadata": {},
   "source": [
    "The CatBoost Regressor gives the best results, as expected! However, we do have to note that the standard deviation for the scores are a little higher than that of the XGBoost Regressor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecead90",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "In order to optimize these algorithms properly, we can tune the parameters of the different models and create even better predictions from this. We will be using RandomizedSearchCV and GridSearchCV in order to tune these parameters. \n",
    "\n",
    "We will be using the full_train and the test dataset for the hyperparameter tuning, we're really justt using the \n",
    "\n",
    "linreg, ridge, rfc, xgb, catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a91aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589cf09",
   "metadata": {},
   "source": [
    "## GridSearchCV vs RandomizedSearchCV\n",
    "\n",
    "GridSearchCV and RandomizedSearchCV are two cross validation methods whereby you can tune your parameters. GridSearchCV basically uses a dictionary of parameters and tries out every single combination in that dictionary. It will give best values like best_estimator, best_score, and most important best_params_. This helps if we are trying to optimise the parameters and we have a range of the best parameters. \n",
    "\n",
    "RandomizedSearchCV is the same, except that it does not try out every single combination in the dictionary, and instead samples a few number of parameters from the dictionary itself. This helps if we are just trying to get a rough estimate for the parameters we're trying to finetune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302888c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full_train = pd.concat([y_train, y_val])\n",
    "\n",
    "del data_full_train['totalyearlycompensation']\n",
    "\n",
    "dicts = data_full_train.to_dict(orient = 'records')\n",
    "dv = DictVectorizer(sparse = False)\n",
    "X_full_train = dv.fit_transform(dicts)\n",
    "\n",
    "dicts = data_test.to_dict(orient = 'records')\n",
    "X_test = dv.fit_transform(dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb9d82",
   "metadata": {},
   "source": [
    "## Linear Regression parameters \n",
    "\n",
    "There aren't a lot of parameters to play with for the original linear regression algorithm, therefore we really should just look at a few parameters here.\n",
    "\n",
    "These parameters are mainly fit_intercept and normalize. Just for fun, let's do a grid search on these parameters since there aren't many things to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'fit_intercept':(True, False), 'normalize': (True, False)}\n",
    "\n",
    "clf = GridSearchCV(linreg, parameters)\n",
    "clf.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2996f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac062f",
   "metadata": {},
   "source": [
    "Just as expected, the best parameters are the default ones. Therefore, we will not need to train the linear regression model again with the optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b46cd",
   "metadata": {},
   "source": [
    "## Random Forest Regression \n",
    "\n",
    "The random forest regressor has more parameters to tune, as compared to linear regression. From here on, we will be using RandomizedSearchCV first before we further tune it with GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "parameters = {\n",
    "    'n_estimators': [10, 100, 200, 350, 500, 700, 850, 1000],\n",
    "    'max_depth':[None, 5, 10, 20, 30, 37],\n",
    "    'min_samples_split':[2, 10, 40, 100, 200, 300, 500, 1000],\n",
    "    'min_samples_leaf':[1, 2, 10, 40, 100, 200, 300, 500, 1000],\n",
    "    'bootstrap':(True, False),\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(rfr, parameters)\n",
    "clf.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b8731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afe504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "\n",
    "rfr = RandomForestRegressor(bootstrap = True, max_depth = None)\n",
    "parameters = {\n",
    "    'n_estimators': [150, 170, 190, 200, 210, 230, 250],\n",
    "    'min_samples_split': [150, 170, 190, 200, 210, 230, 250],\n",
    "    'min_samples_leaf': [150, 170, 190, 200, 210, 230, 250]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(rfr, parameters)\n",
    "clf.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e381d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_params = clf.best_params_\n",
    "rfr_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de3f05",
   "metadata": {},
   "source": [
    "## XGBoost Parameters \n",
    "\n",
    "Like the random forest regressor, there are more parameters to tune. I have defined a new XGBRegressor with the tree_method as gpu_hist so that I am able to use my graphics card for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(tree_method = 'gpu_hist')\n",
    "\n",
    "parameters = {\n",
    "    'eta': [0.001, 0.01, 0.1, 0.3, 0.5, 1, 2, 5],\n",
    "    'max_depth':[None, 5, 10, 20, 30],\n",
    "    'min_child_weight': [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100]\n",
    "    \n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(xgb, parameters)\n",
    "clf.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc22a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c53624",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'min_child_weight': [1.5, 2, 2.5, 3, 4],\n",
    "              'max_depth': [6, 8, 10, 12, 14, 16],\n",
    "              'eta': [0.05, 0.075, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "clf = GridSearchCV(xgb, parameters)\n",
    "clf.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c5a3d",
   "metadata": {},
   "source": [
    "## CatBoostParameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr = CatBoostRegressor(task_type = 'GPU')\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.3, 0.5, 1, 2, 5],\n",
    "    'l2_leaf_reg': [0.001, 0.01, 0.1, 1, 2, 5, 10],\n",
    "    'bagging_temperature': [0,1],\n",
    "    'depth':[1,2,3,4,5,6,7,8]\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(cbr, parameters)\n",
    "clf.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b365fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2021d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr = CatBoostRegressor(task_type = 'GPU', depth = 2)\n",
    "\n",
    "parameters = {'learning_rate': [0.01, 0.02, 0.05, 0.07],\n",
    "              'l2_leaf_reg': [0.001, 0.002, 0.003, 0.004, 0.005],\n",
    "              'bagging_temperature': [1, 2, 3, 4, 5, 6]}\n",
    "\n",
    "clf = GridSearchCV(cbr, parameters)\n",
    "clf.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac338a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8bb8e",
   "metadata": {},
   "source": [
    "## Training the models - round 2\n",
    "We will be training the models further, now with the optimised hyperparameters. We will be using the same models, just that we are training the models with X_full_train and y_full_train in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929dcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "rfr = RandomForestRegressor(bootstrap = True, max_depth = None, \n",
    "                            min_samples_leaf= 250, min_samples_split = 210, n_estimators = 230)\n",
    "xgb = XGBRegressor(tree_method = 'gpu_hist', min_child_weight= 2.5, max_depth= 16, eta= 0.075)\n",
    "cbr = CatBoostRegressor(task_type = 'GPU', depth = 2, learning_rate = 0.01, bagging_temperature = 3, l2_leaf_reg = 0.002, silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "models= [linreg, rfr, xgb, cbr]\n",
    "for model in models:\n",
    "    model.fit(X_full_train, y_full_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = rmse(y_pred, y_test)\n",
    "    print(\"For model %s: RMSE is : %.3f\" % (model, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50468998",
   "metadata": {},
   "source": [
    "It seems like when we use the X_full_train and X_test, the RMSE for RandomForestRegressor and XGBRegressor are a bit higher than that of the CatBoostRegressor and LinearRegression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f8b28",
   "metadata": {},
   "source": [
    "## End of Notebook No.2 \n",
    "\n",
    "Thank you for reading notebook no.2 for training the models and hyperparameter tuning. In the next few python files, I will be deploying the CatBoostRegressor model to the cloud! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
